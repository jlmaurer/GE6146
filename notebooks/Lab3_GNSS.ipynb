{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Global Navigational Satellite Systems (GNSS)\n",
    "\n",
    "In this lab we will explore various applications of GPS/GNSS, one of the most important geodetic instruments to ever be developed. We will look at single point positioning, accessing and interpreting GNSS position time-series, and analyzing velocities to compute strain rate and fault locking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Surveying and Single-point positioning\n",
    "Since we cannot meet together to look at our survey-grade GPS system, we will get some understanding of positioning using our phones. We will use a smartphone app (see __[here](https://play.google.com/store/apps/details?id=eu.basicairdata.graziano.gpslogger&hl=en_US&gl=US)__ for Android phones, try __[here](https://apps.apple.com/us/app/simple-gps-logger/id852832526)__ or __[here](https://apps.apple.com/us/app/mytracks-the-gps-logger/id358697908)__ for Iphones) to access our phones' GPS capabilities. Please download an app appropriate for your phone (or a friend's phone if you do not have a smartphone). Make sure that the app you choose can export your coordinates to a file, and make sure that when logging you can see how the position estimate changes in time (should be small fluctuations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPS \"Surveying\" with a smartphone\n",
    "1. Find a stable point outside where you can place your phone.\n",
    "2. Record the location for:\n",
    "    - A minimum of 3 minutes at a time\n",
    "    - At least two different times of day\n",
    "3. Your \"benchmark\" should be level, outside, and away from tall trees or buildings.\n",
    "4. Save your measurements to a file to be uploaded here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You will want to save the file in a format that can be read into Python. For example, the GPSLogger app on Android can save files in comma-delimited format, which can be read using Pandas directly. (You can change the file format in the app settings.) If your logger app of choice saves files to a different format, you will need to convert the measurements to a .csv file in order to continue with the homework. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Upload your data files using your method of choice ###\n",
    "\n",
    "### Direct upload ###\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "\n",
    "### Use Google Drive ###\n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With either option, you can then import the data using pandas, for example: \n",
    "#import pandas as pd\n",
    "#Nskip = 0 # change to number of rows to skip\n",
    "#filename = \"your_imported_filename.csv\" # need to include /gdrive/ in the filename if importing from google drive\n",
    "#df = pd.read_csv(filename, skiprows=Nskip) # you will need to skip the header rows. Look at the file on your computer to see how many rows you need to skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> When reading your data file, if the first row contains the names of the columns, you do not need to skip any rows. Also, for the GPS data in your file, you will want to have at least three or four columns, one with time and two with Lat and Lon, with an optional height column. Your logger app may save other columns as well but they are not needed for this lab. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to combine all your measurements (for the 2+ recording sessions you did) into a single pandas dataframe for analysis. You can do this by uncommenting and running using the following cell (edit the cell to use the proper names of your input dataframes from the cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine a list of dataframes into a single dataframe. If you have more than two, just add them to the list. \n",
    "# the dataframes in df_list must have the same columns\n",
    "# df_list = [df_1, df_2]\n",
    "# df_total = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the following plots of your GPS data. Make sure to include axis labels, legends, titles, and colorbars as needed, and to make sure that the data is well visible (i.e. zoom in or out if needed). \n",
    "1. Create a 2-D scatterplot (point cloud) of latitude vs longitude. \n",
    "2. (Optional) Create a 3-D scatterplot with lat, lon, and height.\n",
    "3. Compute and subtract out the mean lat/lon/height and plot the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for plot 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for plot 2 (Optional +2 points on the homework score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for plot 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "In the cell below, write your answers to the following questions. If you need to, add a cell with \"Markdown\" for writing non-code answers. Don't forget you can use Python as a calculator, and that pandas has a \".mean()\" and \".std()\" methods to help with calculating means and standard deviations. \n",
    "1. What is your average position (decimel degrees latitude and longitude, and decimel height in meters if you did height)?\n",
    "2. What is the average scatter around your measurements? Report your results as the standard deviation in latitude, longitude, and height? What are the units of the standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Accuracy\n",
    "What would you say is the overall precision of your measurements in meters? I.e. your overall positional precision is +/- * meters. To answer this question:\n",
    "1. First convert your residual lat/lon measurements (i.e., the lat/lon measurements you took minus the mean lat/lon position) to meters. You will need to search for how many meters is a degree of latitude and longitude for your location (both change somewhat, longitude changes a lot depending on your location on earth). \n",
    "2. Create a scatter plot of the residual measurements in meters\n",
    "3. Estimate the standard deviation of the residuals for latitude and longitude separately\n",
    "4. Compute the mean residual norm (MRN, or overall accuracy) using the equation: \n",
    "$$ \n",
    "MRN = \\sqrt{(2*r_{N})^2 + (2*r_{E})^2}\n",
    "$$\n",
    "where $r_N$ is the latitude standard deviation in meters, and $r_E$ is the longitude standard deviation in meters. Your units will be in meters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Finding Location and Velocity data\n",
    "\n",
    "In this part of the lab we will learn about accessing GNSS time-series and velocity data. GNSS data is publicly available from __[UNAVCO](https://www.unavco.org/)__, a non-profit university-governed consortium that facilitates geoscience research and education using geodesy, and from the University of Nevada-Reno's __[MAGNET](http://geodesy.unr.edu/NGLStationPages/gpsnetmap/GPSNetMap_MAG.html)__ database. In this homework, you will download, plot, and analyze GNSS time-series from several stations. \n",
    "\n",
    "Original activity by Vince Cronin (Baylor University). Revisions by Beth Pratt-Sitaula (UNAVCO) and Jeremy Maurer (Missouri S&T)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analyzing the velocities recorded at different GPS stations can give significant insights into plate tectonic motion, earthquake hazards, volcanic hazards, groundwater removal, and more.*\n",
    "\n",
    "GPS data can be acquired from a variety of different research groups around the world, but some the most accessible and easy to use GPS data comes from the EarthScope Plate Boundary Observatory (PBO), which is managed by UNAVCO. The data are available online for free at __[here](https://www.unavco.org/instrumentation/networks/status/pbo/gps)__. In this exercise you will learn one method for downloading GPS station location and velocity data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw GNSS data\n",
    "In this lab we will not work with the raw GNSS data; this is because GNSS data for tectonic purposes is usually processed as a *network*, meaning a group of stations are processed all at once to obtain position information. This is different than what your phone or a typical GNSS receiver does, which is to estimate your position based on a single receiver. Network processing allows for much more precise positioning, because it allows the processing algorithm to solve for certain noise sources that are common to all stations and subtract it out. The image below shows an example of a Rinex file for GNSS station \"CAPO\" located in Haiti using a Trimble-brand GNSS receiver.\n",
    "![Rinex_file](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/Rinex_file_example.png)\n",
    "\n",
    "The image below shows a snapshot of the UNAVCO __[Data Archive Interface (DAI)](https://www.unavco.org/data/gps-gnss/data-access-methods/dai2/app/dai2.html#)__, where raw GNSS data can be obtained for any of the station holdings in UNAVCO's archive.\n",
    "![UNVACO_DAI.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/UNVACO_DAI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNSS time-series\n",
    "Most GNSS applications, including the earth sciences, use position time-series. The plot below shows an example time-series for station (P595) in California, near where the 2019 Ridgecrest earthquake occurred. The time-series shows the point location relative to the initial starting point for each day. GNSS records data continously, but the data are typically averaged to one position per day to reduce noise. The time-series provides several types of useful information. First, the points change at a fairly constant rate; in the plot, you can see that the position of the GNSS station moves west (negative east) at a rate of approximately 7 mm/yr, and north at approximately 10 mm/yr. The data shows a fairly constant rate of change until an event, which in this case is the Ridgecrest earthquake. You also see that the vertical motion is much noisier and has\n",
    "a much lower rate (the slope of the average trend is small).\n",
    "![P595.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/P595.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detrended Time-series\n",
    "The next image shows a de-trended time-series, where the average rate has been subtracted from each day's point. The detrended time-series shows the total offset from the earthquake Ridgecrest earthquake, and it shows that the displacements after the earthquake are not zero right away, but there are some transients that take some time to dissipate.\n",
    "![P595_detrended.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/P595_detrended.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked Example: Finding PBO GPS data in the Oregon Coast Ranges\n",
    "## Finding station locations in latitude-longitude coordinates\n",
    " We will search for data generated by one of the PBO’s permanent GPS stations above the Cascadia subduction zone in northwest Oregon. If we do not know which station we want to learn about, we can go to the interactive PBO map and zoom-in on our area of interest (https://www.unavco.org/instrumentation/networks/status/pbo/gps). We find several green marker dots along the coastline. Clicking on any of the dots will provide some initial information. The dot we chose (Figure 1) is associated with station P395 (Rose_LodgeOR2006) located west of Salem in northwest Oregon. Clicking on the dot gives us a box that provides the name and location of the site, as well as a clickable link to more information at https://www.unavco.org/instrumentation/networks/status/pbo/overview/P395\n",
    "![P395.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/P395.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overview page provides us with some information that we will need, and even some resources we might not need (such as a picture of the station) but is nice to have. \n",
    "![P395_fieldstation.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/P395_fieldstation.png)\n",
    "\n",
    "Near the bottom on the left side of the P395 overview page is a box titled “GPS Monument Coordinates.” The station location is listed on the line that begins “lat/log/elev (d/d/m).” The first number is the site latitude with positive values indicating north latitude (geographic coordinate system in the WGS84 datum). The second number is the longitude, with negative indicating west longitude. The third number is the elevation in meters relative to the WGS84 ellipsoid – that is, it is not the elevation relative to mean sea level, as might be indicated on a USGS topographic map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding station velocities\n",
    "Now we need to acquire the velocity data for the site. Returning to the overview page for site P395, mid-way along the left side of the page there is a box labeled “Station Position” with three graphs. Clicking on the box causes the window to expand. The three plots show the change in position as a function of time – that is, they are time-series plots. The upper plot shows change in a north or south direction, the middle plot shows motion in an east or west direction, and the lowest plot shows motion in the up or down direction. These changes are measured relative to a reference frame that is computed relative to a set of GPS sites established in the stable interior of North America (called NAM14 for “North America 2014). __[Read more about references frames.](http://www.unavco.org/software/visualization/GPS-Velocity-Viewer/GPS-Velocity-Viewer-frames.html)__  \n",
    "\n",
    "You can switch from the raw data view to a “cleaned” view to a “detrended” view using either the navigation arrows on your keyboard or by using the small control bar that should appear near the bottom of the plot window. Both the cleaned and detrended plots provide a numerical estimate of the mean velocity in each of the three directions along with the corresponding uncertainty (Figure 3). These velocities are frequently updated as new data are collected. We can also access the daily position data in a CSV spreadsheet file, and compute our own velocities and uncertainties.\n",
    "\n",
    "On December 5, 2017, the posted velocities were:\n",
    "- North 8.66 ± 0.08 mm/yr (positive value indicates motion toward north)\n",
    "- East 6.92 ± 0.18 mm/yr (positive value indicates motion toward east)\n",
    "- Up 0.46 ± 0.30 mm/yr (negative value indicates motion down)\n",
    "\n",
    "We can use these data and the Pythagorean Theorem to find the total site velocity in mm/yr and the mean horizontal velocity in mm/yr: \n",
    "$$\n",
    "v_{total} = \\sqrt{v_E^2 + v_N^2 + v_U^2}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "v_{horiz} = \\sqrt{v_E^2 + v_N^2}\n",
    "$$\n",
    "\n",
    "Carefully record the velocities and their associated uncertainties for GPS stations P395, P396, and P404 below. The signs of the velocities are important because they indicate the direction of the velocity vector. \n",
    "![P395_detrended.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/P395_detrended.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the station locations as [lon, lat, hgt] vectors\n",
    "xyz_p395 = np.array([])\n",
    "xyz_p396 = np.array([])\n",
    "xyz_p404 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the velocities as a single three-component vector for each station (Veast, Vnorth, Vup)\n",
    "v_p395 = np.array([])\n",
    "v_p396 = np.array([])\n",
    "v_p404 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total and horizontal velocity magnitudes for each station using the equations above \n",
    "\n",
    "# an example start for the total velocity. For the horizontal, just use the horizontal components of the vector\n",
    "def computeTotalVelocity(velVector):\n",
    "    '''\n",
    "    Function to compute the total velocity (magnitude) of an input velocity vector\n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    # Square each element of the input vector\n",
    "    # your code here\n",
    "    \n",
    "    # Take the sqrt of the sum\n",
    "    totalVelocity = 0. # change this\n",
    "    \n",
    "    # return the result\n",
    "    return totalVelocity\n",
    "\n",
    "# total velocities\n",
    "vt_p395 = 0.\n",
    "vt_p396 = 0.\n",
    "vt_p404 = 0.\n",
    "\n",
    "# horizontal velocities\n",
    "vt_p395 = 0.\n",
    "vt_p396 = 0.\n",
    "vt_p404 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B Optional Advanced Problem for Extra Credit (Bonus 5 points)\n",
    "If you are interested, you can compute the components of the strain rate tensor. To do this, you will need to determine the line-length directions and changes between each pair of stations, then use the equations below to compute the components of the strain rate tensor. $\\Delta \\mathcal{l}_i$ is the line length change along the direction $\\alpha$ for the $i$th station pair (three stations in a triangle = three pairs of stations = three lines). You will need to setup the matrix-vector linear system and solve it for the strain rate components. You can use the linear solver example from Part C to help you do this in Python if you want. \n",
    "![strain_from_dlength.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/strain_from_dlength.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: GNSS for Tectonic Deformation and Hazard\n",
    "\n",
    "In this lab we will consider what Global Navigational Satellite Systems (GNSS) data tell us about earthquake hazard on the central San Andreas Fault. In particular, we will consider the difference in the geodetic signatures of locked and creeping faults. \n",
    "\n",
    "## An important lesson from a major catastrophe\n",
    "In 1906 the great San Fransisco earthquake occurred and resulted in the destruction of much of San Fransisco (although the largest amount of damage was actually due to the fires that burned as a result of broken gas lines and no water to put them out because of broken water lines). I encourage you to take a look at the __[Wikipedia](https://en.wikipedia.org/wiki/1906_San_Francisco_earthquake)__ description of the event, which describes in detail the aftermath of the event itself and the national and international repercussions in the economy and finance industry of the time. Should such an earthquake happen again today, there would likely be some improvement in damaged buildings, but many of the cities' older buildings and sectors, including gas lines, are still vulnerable. \n",
    "\n",
    "The size of the earthquake is uncertain due to the lack of good observations from that time; however, scientists at the United States Geological Survey (USGS) have __[estimated that the earthquake was an MW 7.7 or 7.9](https://earthquake.usgs.gov/earthquakes/events/1906calif/18april/magnitude.php)__. \"MW\" stands for \"moment magnitude,\" and is different from the \"Richter magnitude.\" The Richter magnitude scale is no longer used much by scientists because it describes the intensity of shaking, rather than the actual size of the earthquake, and so it can vary a bit depending on site conditions (e.g., soft soils shake harder than bedrock). \n",
    "\n",
    "However, the __[lessons learned from the 1906 earthquake](https://earthquake.usgs.gov/earthquakes/events/1906calif/18april/revolution.php)__ are still applied today. The __[**theory of elastic rebound**](https://earthquake.usgs.gov/earthquakes/events/1906calif/18april/reid.php)__, which we will use in this lab, is a fundamental idea about how earthquakes work. Why is the theory important? Because it allows us to make predictions of how much slip will happen in the next earthquake. By measuring how much of a fault is \"locked,\" we can turn that around to say how large the next earthquake is likely to be. We will do this in this lab.\n",
    "\n",
    "## The San Andreas Fault\n",
    "The San Andreas Fault (abbreviated SAF) is the largest strike-slip fault system in the United States, and produced both the MW 7.7 San Fransisco earthquake as well as the MW 7.9 Fort Tejon earthquake that occurred in southern California. We will be looking at data around the SAF at the northern end of the Fort Tejon earthquake rupture, near Parkfield, CA. North of this area the SAF is creeping at the surface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNSS time-series\n",
    "Processing raw GNSS data for tectonic studies is a highly technical skill that we will not be able to address in this course. Instead, we will access publicly-available data that has already been processed up to the time-series level. This time-series data contains one position point in three dimensions for a given station on a daily frequency. The data is available from several different sources, including the __[University of Nevada at Reno](http://geodesy.unr.edu/NGLStationPages/gpsnetmap/GPSNetMap_MAG.html)__ and __[UNAVCO](https://www.unavco.org/data/gps-gnss/data-access-methods/data-access-methods.html)__; the latter has data from several different sources. Using data already pre-processed will save a lot of time for this lab, but be aware that each processing agency has its own set of stations and quirks that must be investigated for real research-level work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the libraries and set up some helpful functions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# some helpful functions\n",
    "def cosd(angle_in_degrees):\n",
    "    return np.cos(np.radians(angle_in_degrees))\n",
    "def sind(angle_in_degrees):\n",
    "    return np.sin(np.radians(angle_in_degrees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need these libraries to run this notebook\n",
    "#!pip install pyproj #<-- Uncomment and run this once at the beginning of running this section of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some GNSS velocities \n",
    "URL = 'https://raw.githubusercontent.com/jlmaurer/GE6146/master/data/GNSS_Vels.csv'\n",
    "gnss_vels_Parkfield = pd.read_csv(URL).set_index('SITE_NAME')\n",
    "gnss_vels_Parkfield['Lon'] = gnss_vels_Parkfield['Lon'] - 360  # convert to degrees west"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fields Veast, Vnorth, Vup, sigEast, sigNorth, and sigUp are all given in meters. To make the numbers easier to work with, convert them to millimeters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Veast</th>\n",
       "      <th>Vnorth</th>\n",
       "      <th>Vup</th>\n",
       "      <th>sigEast</th>\n",
       "      <th>sigNorth</th>\n",
       "      <th>sigUp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITE_NAME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALTH</th>\n",
       "      <td>36.889607</td>\n",
       "      <td>-120.669763</td>\n",
       "      <td>-0.00931</td>\n",
       "      <td>0.01036</td>\n",
       "      <td>-0.04385</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.00039</td>\n",
       "      <td>0.00765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALTH</th>\n",
       "      <td>36.889607</td>\n",
       "      <td>-120.669763</td>\n",
       "      <td>-0.00931</td>\n",
       "      <td>0.01036</td>\n",
       "      <td>-0.04385</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.00039</td>\n",
       "      <td>0.00765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKR1</th>\n",
       "      <td>35.132389</td>\n",
       "      <td>-119.109480</td>\n",
       "      <td>-0.01356</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>-0.03680</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.00844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKR1</th>\n",
       "      <td>35.132388</td>\n",
       "      <td>-119.109480</td>\n",
       "      <td>-0.01356</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>-0.03680</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.00844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BVPP</th>\n",
       "      <td>35.157277</td>\n",
       "      <td>-119.347509</td>\n",
       "      <td>-0.01736</td>\n",
       "      <td>0.01510</td>\n",
       "      <td>-0.00046</td>\n",
       "      <td>0.00035</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.00134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Lat         Lon    Veast   Vnorth      Vup  sigEast  \\\n",
       "SITE_NAME                                                              \n",
       "ALTH       36.889607 -120.669763 -0.00931  0.01036 -0.04385  0.00113   \n",
       "ALTH       36.889607 -120.669763 -0.00931  0.01036 -0.04385  0.00113   \n",
       "BKR1       35.132389 -119.109480 -0.01356  0.01318 -0.03680  0.00083   \n",
       "BKR1       35.132388 -119.109480 -0.01356  0.01318 -0.03680  0.00083   \n",
       "BVPP       35.157277 -119.347509 -0.01736  0.01510 -0.00046  0.00035   \n",
       "\n",
       "           sigNorth    sigUp  \n",
       "SITE_NAME                     \n",
       "ALTH        0.00039  0.00765  \n",
       "ALTH        0.00039  0.00765  \n",
       "BKR1        0.00113  0.00844  \n",
       "BKR1        0.00113  0.00844  \n",
       "BVPP        0.00019  0.00134  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the data\n",
    "gnss_vels_Parkfield.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "1. These velocities are from UNAVCO (see website link above) and are in the North American 2014 reference frame\n",
    "2. \"Veast\", \"Vnorth\", \"sigEast\", etc are velocities and uncertainties in the East, North, and Up directions.\n",
    "3. Having three velocity components (East, North, Up) means that these GNSS velocities are actually 3-component\n",
    "    vectors, which can be represented by: \\begin{bmatrix}Veast \\\\ Vnorth\\\\ Vup\\end{bmatrix}. \n",
    "4. The uncertainties shown are only for the individual components. More generally, the uncertainty of every individual velocity measurement has a 3x3 matrix of associated uncertainties: \\begin{bmatrix} \\sigma^2_{E} & \\sigma^2_{EN} & \\sigma^2_{EU} \\\\ \\sigma_{NE} & \\sigma^2_{NN} & \\sigma_{NU} \\\\ \\sigma^2_{UE} & \\sigma^2_{UN} & \\sigma^2_{UU} \\end{bmatrix}\n",
    "For the values we have been provided, we can only fill in the diagonal elements of the matrix, and we will assume the others are zero: \\begin{bmatrix} \\sigma^2_{E} & 0 & 0 \\\\ 0 & \\sigma^2_{N} & 0 \\\\ 0 & 0 & \\sigma^2_{U} \\end{bmatrix} where \"sigEast\" from the table is equal to $\\sqrt{\\sigma^2_{E}}$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the data** using the __[quiver](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.quiver.html)__ package in matplotlib.pyplot. Make sure to use \"Lon\" for the x-axis and \"Lat\" for the y-axis. Use Veast for the U-component of velocity and Vnorth for the V-component. Make sure to add appropriate titles, axis labels, legends, etc. as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shows some of these same stations plotted in the UNAVCO GNSS viewer to give you a better idea of where they are. The view is extended to Los Angeles for reference. \n",
    "![UNAVCO_GNSS_viewer.png](https://raw.githubusercontent.com/jlmaurer/GE6146/master/notebooks/images/UNAVCO_GNSS_viewer.png)\n",
    "\n",
    "In order to better view and model these data, we will first rotate them into a coordinate system that is parallel to the San Andreas Fault. This way we can model the data as a 1-D velocity profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = \n",
      "[[ 0.76604444  0.64278761  0.        ]\n",
      " [-0.64278761  0.76604444  0.        ]\n",
      " [ 0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to create a rotation matrix\n",
    "def createRotationMatrix(angle_deg):\n",
    "    RotationMatrix = np.array([[cosd(angle_deg), sind(angle_deg), 0], [sind(-angle_deg), cosd(angle_deg), 0], [0, 0, 1]])\n",
    "    return RotationMatrix\n",
    "\n",
    "'''\n",
    "The rotation matrix is a 3x3 matrix that will rotate the velocities into a fault-parallel/fault perpendicular\n",
    "coordinate system and leave the vertical heights unchanged.\n",
    "'''\n",
    "rotation_angle = 40 \n",
    "R = createRotationMatrix(rotation_angle) # we will call the matrix R in lower cells\n",
    "print('R = ')\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will iterate over each station and rotate each set of velocities individually.\n",
    "# We don't need the verticals for this part\n",
    "Vparr, Vperp= [],[]\n",
    "for ve, vn, vu in zip(gnss_vels_Parkfield['Veast'], gnss_vels_Parkfield['Vnorth'], gnss_vels_Parkfield['Vup']):\n",
    "    vOriginal = np.array([ve, vn, vu])\n",
    "    vRotated = np.dot(R, vOriginal)\n",
    "    Vparr.append(vRotated[1])\n",
    "    Vperp.append(vRotated[0])\n",
    "\n",
    "Vparr = np.array(Vparr) # This converts the list we created using the for-loop to numpy arrays (matrices)\n",
    "Vperp = np.array(Vperp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In order to model the velocities, we need to convert the data into a local coordinate system.\n",
    "We will convert the lat/lon points to UTM (x,y) and then subtract out a reference point located \n",
    "very close to the fault. You will need to find the UTM zone for the data online (use the plot showing \n",
    "lat/lon above) \n",
    "'''\n",
    "from pyproj import Transformer\n",
    "\n",
    "# setup your projections\n",
    "crs_wgs = 'epsg:4326' # WGS84 geographic\n",
    "crs_local = 'epsg:26911' # use a locally appropriate projected CRS\n",
    "transformer = Transformer.from_crs(crs_wgs, crs_local, always_xy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast your geographic coordinate pair to the new projected system\n",
    "input_lon = gnss_vels_Parkfield['Lon'].values # \".value\" is necessary to convert from a pandas Series to a numpy array\n",
    "input_lat = gnss_vels_Parkfield['Lat'].values\n",
    "X, Y = transformer.transform(input_lon, input_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now write a code that takes an origin point (use 36.0, -120.5 as your origin) and converts \n",
    "it to the same local projection as above. \n",
    "'''\n",
    "origin = np.array([36.0, 239.5])\n",
    "\n",
    "# Call your new variables X and Y\n",
    "xOrigin, yOrigin = transformer.transform(origin[1], origin[0])\n",
    "\n",
    "# You need to change the code below to subtract the origin x,y from the x,y points of all the data\n",
    "X = X #This is not correct\n",
    "Y = Y #This is not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use the for-loop above as a template to rotate the local x,y coordinates\n",
    "Xparr, Xperp= [],[]\n",
    "for x, y in zip(X, Y):\n",
    "    xyOriginal = np.array([x,y,0]) # we will assume heights are all zero; for our purposes it doesn't matter\n",
    "    # Your code here\n",
    "\n",
    "Xparr = np.array(Xparr)\n",
    "Xperp = np.array(Xperp)\n",
    "\n",
    "# Convert the new coordinates to km instead of meters \n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the new velocities at the new locations using quiver\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "In order to understand what the velocities tell us about fault locking, we need to use a model. For this simple excercise, we will use a single fault with only two parameters: slip rate and locking depth. In this case, assuming an elastic half-space, the velocities across the fault can be modeled with the following equation: \n",
    "$$\n",
    "\\mathbf{v}_3(x_2 = 0) = \\frac{\\dot{s}}{\\pi} \\tan^{-1}{\\left(\\frac{x_1 - x_F}{d}\\right)}\n",
    "$$\n",
    "where $\\dot{s} = ds/dt$ is the fault slip rate, the surface velocities $v_3$ are in the out-of-plane direction $x_3$, $x_1$ is the fault-perpendicular position, $x_F$ is the $x_1$ location of the fault, and $d$ is the maximum depth of the fault (or \"locking depth\"). There are two or three unknowns in this equation: $\\dot{s}$ and $d$, and $x_F$ if you do not know it. For us, \"Vparr\" is $v_3$ and \"Xperp\" is $x_1$. ($x_2 = 0$ means that our measurements are taken at the surface of the ground.) With this information you should be able to model the velocities you plotted in the last cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> One more issue you will have to resolve is that there could be an unkonwn offset, termed a \"rigid-body\" offset, that relates to the frame of reference of the velocities. This means that you could add or subtract a constant term (scalar) from all of the measurements and not change the strain or estimated slip rate or depth. This effect is termed \"rigid body\" because it is a translation - a movement of all points in the same direction for the same distance - with no strain. Mathematically, this acts like a constant add to the equation above, which is also unknown.\n",
    "$$\n",
    "\\mathbf{v}_3 = \\frac{\\dot{s}}{\\pi} \\tan^{-1}{\\left(\\frac{x_1 - x_F}{d}\\right)} + c\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving for the $c$ and $x_F$ terms are relatively easy, because they just adjust your model up or down ($c$) or left-to-right ($x_F$). You can estimate what they should be by simply looking at the plot. For $\\dot{s}$ and $d$, we will use a non-linear Greens function approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if $x_F$ and $d$ are known, we can write the equation as a linear system: \n",
    "$$\n",
    "\\mathbf{v}_{parr} = \\begin{bmatrix} \\frac{1}{\\pi} \\tan^{-1}{\\left(\\frac{x_1 - x_F}{d}\\right)} & 1 \\end{bmatrix} \\begin{bmatrix} \\dot{s} \\\\ c \\end{bmatrix}\n",
    "$$\n",
    "This is a dot product of two length-two vectors. If this were the only step, our job would be easy, as there are many linear solvers for handling dot product-type problems. However, because $x_F$ and $d$ are inside the left matrix (our Greens' function matrix); note that there is one row for each data point, so even though I've written it as a vector it is actually a M x 2 matrix, where M = length(Vparr). In order to solve this problem, we will take a sampling approach, where we take a set of values for $d$ and compute the overall data misfit. We will then take as our solution the value with the lowest misfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of x_1-values \n",
    "xMax = 130 # Take the maximum and minimum x-values from the data (note units are in km)\n",
    "xMin = -80\n",
    "Nx = 1000\n",
    "xs = np.linspace(xMin, xMax, Nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some d-values\n",
    "ds = np.arange(5, 26, 1)  # Create a set of depths from 5 to 20 km, typical for California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the position parameter as best you can\n",
    "xF = 0. # Your value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the G-matrices for each d\n",
    "Gs = []\n",
    "for d in ds:\n",
    "    term1 = None # Your code for the arctanget term here\n",
    "    term2 = np.ones(term1.shape)\n",
    "    Gs.append(np.stack([term1, term2], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows of a G matrix should equal the number of observations, in this case Vparr, and the number of columns should equal the number of unknown parameters, in this case two: $\\dot{s}$ and $c$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(268, 2)\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "print(Gs[0].shape)\n",
    "print(len(Vparr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate over the list of G-matrices to create the residual misfit norm for each value of d. We will use a numpy algorithm called lstsq (least-squares) to solve the linear system given $d$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each G and solve for sdot\n",
    "sdots, cs = [], [] # create two lists to store the solutions\n",
    "res_norms = [] # create a list to store the residual misfit norms\n",
    "for G in Gs:\n",
    "    mhat, res, rank, s = np.linalg.lstsq(G, Vparr, rcond=None)\n",
    "    sdots.append(mhat[0])\n",
    "    cs.append(mhat[1])\n",
    "    \n",
    "    # compute predicted data and residual misfits\n",
    "    predictedData = np.dot(G,mhat)\n",
    "    residuals = Vparr - predictedData\n",
    "    res_norm = np.sum(np.square(residuals))/len(Vparr)\n",
    "    res_norms.append(res_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the total misfit versus d\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the estimated slip rate versus d\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predicted data for the best-fitting (or your preferred choice of) parameter combination\n",
    "# You may need to experiment with various parameter values here to find the \"best\" one from your perspective\n",
    "dBest = 0. # pick the best d\n",
    "xF = 0. # adjust xF if needed\n",
    "\n",
    "term1 = (1/np.pi)*np.arctan2(Xperp - xF, dBest)\n",
    "term2 = np.ones(term1.shape)\n",
    "Gbest = np.stack([term1, term2], axis=1)\n",
    "\n",
    "mbest, res, rank, s = np.linalg.lstsq(Gbest, Vparr, rcond=None)\n",
    "dpred_best = np.dot(Gbest, mbest)\n",
    "residuals_best = dpred_best - Vparr\n",
    "rnorm_best = np.sum(np.square(residuals_best))/len(Vparr)\n",
    "\n",
    "# Plot the observed and predicted data in space\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preferred locking depth is 0.0, and slip rate is 0.0 mm/yr\n"
     ]
    }
   ],
   "source": [
    "# Based on above, print out your preferred slip rate and locking depth\n",
    "dbest = 0. # your preferred locking depth\n",
    "sdot_best = 0. # your preferred slip rate\n",
    "print('The preferred locking depth is {}, and slip rate is {} mm/yr'.format(dbest, sdot_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
